{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcad13-510a-4bfd-81ea-067c0fc248e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927b4ed-3a39-44c8-84ac-00c29ebc5623",
   "metadata": {},
   "source": [
    "# Step 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452365d8-0175-4a18-bdc1-e4b47c806bcc",
   "metadata": {},
   "source": [
    "First, we need to prepare our dataset. The dataset consists of images of different dog breeds that are located in separate folders according to their respective breeds. In our example, we assume that the directory of our dataset is dogs/ and each dog breed is in a separate folder within that directory. Here is how you can load the dataset using PyTorch's torchvision library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad3fda-cc1b-4851-8da6-a893718c42c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import os\n",
    "\n",
    "if \"win\" not in platform:\n",
    "    if not os.path.exists(\"dogs/\"):\n",
    "        !unzip dogs.zip\n",
    "else:\n",
    "    print(\"Your OS is Windows, unzip dogs.zip manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cec548-ad25-448f-a55f-49f4bf9e1c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define the directories path where the data is stored\n",
    "train_data_dir = 'dogs/Training'\n",
    "test_data_dir = 'dogs/Test'\n",
    "\n",
    "# Load the dataset using PyTorch's torchvision.datasets.ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54cedc-7f9e-4608-a0c6-0219d3c34071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "class_names = [i.stem for i in list(pathlib.Path('dogs/Training').iterdir())]\n",
    "class_names = pd.Series(class_names).apply(lambda x: x.split('-')[1]).tolist()\n",
    "class_idx = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "\n",
    "example = train_dataset[620]\n",
    "plt.imshow(example[0]) # index 0 contains the image\n",
    "plt.title(f\"LABEL ID={example[1]}, NAME={class_idx[example[1]].split('-')[1]}\"); # index 1 contains the label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80561e5e-3c30-4e77-94cc-f79617a44e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae6b2c-ff99-4f14-99bc-8edc1e7128cf",
   "metadata": {},
   "source": [
    "Before training the model, we need to preprocess our data to normalize the pixel values and resize the images to a consistent size. We can use PyTorch's transforms module to apply the necessary preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3e609-c702-4b16-81c1-6135488db0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply the transformation pipeline to our dataset\n",
    "train_dataset = datasets.ImageFolder(train_data_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a243-a534-4740-9502-f39fbc195b4d",
   "metadata": {},
   "source": [
    "Here, we resize the images to a 224x224 resolution, convert them to PyTorch tensors, and normalize the pixel values using the mean and standard deviation of the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deea2e5-1e48-40f9-8c54-ecb82c66c481",
   "metadata": {},
   "source": [
    "# Step 3: Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aebdfc-49b9-4de8-84d2-2eea1a01f1df",
   "metadata": {},
   "source": [
    "To train our model, we need to split our dataset into training, validation, and test sets. We can use PyTorch's random_split function to split the dataset into these sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec496c3-aa52-4e6b-9ed2-22724b37a08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define the sizes of the training, validation, and test sets\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = int(0.3 * len(train_dataset))\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "# Split the dataset randomly into training, validation, and test sets\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ee92b-564b-4549-a6c2-efc1703d641b",
   "metadata": {},
   "source": [
    "In this example, we are using a 70-30 split for our training and validation. We keep the test set separate since we have a separate folder for it (`'dogs/Test'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507c895-7573-47cd-be75-ceffaf8ca846",
   "metadata": {},
   "source": [
    "# Step 4: Creating Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd724d1-2409-4db6-a7ae-4f41efd40f4c",
   "metadata": {},
   "source": [
    "Next, we need to create data loaders to feed our data to the model during training. Data loaders are a PyTorch abstraction that helps us load data efficiently by prefetching batches in parallel with the model's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634fbea-bc47-4b72-bcee-1f75cf8da9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the batch size for the data loaders\n",
    "batch_size = 16\n",
    "\n",
    "# Create data loaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6d9a3-e4f7-4d28-85d1-10d02bac21ec",
   "metadata": {},
   "source": [
    "Here, we define a batch size of 64 and use the DataLoader class to create data loaders for our training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931d2ec-92bd-4615-8096-ef924ac3c58d",
   "metadata": {},
   "source": [
    "# Step 5: Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c5012-8c1b-4cf0-b7c6-28f5ed5d3856",
   "metadata": {},
   "source": [
    "Now, we can define our model architecture. In this example, we will use a pre-trained ResNet-18 model as our feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affed3e0-b125-4421-b7f8-7d80ae38ecdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers in the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer with a new one that outputs the number of classes\n",
    "num_classes = len(train_dataset.dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41eff1-a4d3-446d-b274-dbb0bad43000",
   "metadata": {},
   "source": [
    "In this example, we use a pre-trained ResNet-18 model as our feature extractor and replace the last fully connected layer with a new one that outputs the number of classes in our dataset. We also freeze all the layers in the model except for the new fully connected layer. Finally, we move the model to the GPU if it is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd33be-cdf9-4c31-8a13-1cfb8744fb64",
   "metadata": {},
   "source": [
    "# Step 6: Defining the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da10faa-80f9-4a05-93ac-357b64468336",
   "metadata": {},
   "source": [
    "Before we can train our model, we need to define a loss function and an optimizer. In this example, we will use cross-entropy loss and stochastic gradient descent (SGD) as our loss function and optimizer, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924281dd-51c2-462b-b2b0-6bb85d1347ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244ab01-63e3-4ecd-a88d-2c5291c604f8",
   "metadata": {},
   "source": [
    "Here, we use cross-entropy loss as our loss function and SGD with a learning rate of 0.01 and momentum of 0.9 as our optimizer. We only optimize the parameters in the last fully connected layer of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bccc53-5e09-4510-bc1d-5182a54552e5",
   "metadata": {},
   "source": [
    "# Step 7: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d6a9-5457-4f28-88bb-fbcec1627d76",
   "metadata": {},
   "source": [
    "Now, we can train our model using a loop that iterates over the training data for multiple epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f9c5a-f0e6-4db9-8a57-18d6eb03441f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the number of epochs to train for (increase it)\n",
    "num_epochs = 3\n",
    "\n",
    "# Train the model for multiple epochs\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Total progress (in epochs)\"):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in tqdm(enumerate(train_loader), desc=\"Training (in batches)\", total=len(train_dataset)//batch_size):\n",
    "        # Move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute the average training loss and accuracy\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Compute the validation loss and accuracy\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating (in batches)\", total=len(val_dataset)//batch_size):\n",
    "            # Move the inputs and labels to the GPU if available\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass and prediction\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute the average\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'\\nEpoch {epoch+1} finished! Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46b5af-262c-4fcd-b709-2d468f290b22",
   "metadata": {},
   "source": [
    "Here, we loop over the training data for the specified number of epochs and perform forward and backward passes to update the model parameters. For each batch of data, we move the inputs and labels to the GPU if available, compute the loss, and perform backward propagation and optimization. We also print the running loss every 100 batches to track the progress of the training.  \n",
    "After each epoch, we compute the accuracy of the model on the validation set. We move the inputs and labels to the GPU if available, perform a forward pass to get the model's predictions, and compute the number of correctly predicted samples. We then print the validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba2af6-a26c-460b-a4a9-1e4dee13709e",
   "metadata": {},
   "source": [
    "# Step 8: Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b3fe4-a31f-432d-bc01-913cbc8844ed",
   "metadata": {},
   "source": [
    "Once we have trained our model, we can test it on the test set to see how well it performs on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d890d5-bc6a-4f53-b58e-7e88c6dab41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Performing inference\", total=len(test_dataset)//batch_size):\n",
    "        # Move the inputs and labels to the GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass and prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update total and correct counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print test accuracy\n",
    "print('Test Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab0750-00be-4ee3-bd2c-9a7156069e9b",
   "metadata": {},
   "source": [
    "Here, we loop over the test set and perform a forward pass to get the model's predictions. We then compute the number of correctly predicted samples and print the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ca992-e60f-4a60-b202-91d135d673c7",
   "metadata": {},
   "source": [
    "# Step 9: Computing the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337466b6-881b-4c71-ac73-8aa7e0255eb9",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that shows the performance of a classification model on a set of test data for which the true values are known. It can help us understand which classes our model is performing well on and which classes it is struggling with. Here's how we can compute and plot the confusion matrix for our dog breed classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b41d0-c553-4838-9cae-cd5d0352468c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the true labels for the test set\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Append the true labels to the list\n",
    "        y_true.extend(labels.numpy())\n",
    "        # Forward pass and prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141215c9-c159-416e-9f41-e1eeb12056de",
   "metadata": {},
   "source": [
    "Here, we first loop over the test set and get the predicted labels for each sample. We then get the true labels for the test set. Using these two sets of labels, we can compute the confusion matrix using the confusion_matrix function from the scikit-learn library. Finally, we plot the confusion matrix using a heatmap with the seaborn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8d55d-b15e-46ac-bec7-3f98ce0806f5",
   "metadata": {},
   "source": [
    "# Step 10: Plotting Randomly Selected Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f864ce7-13f8-477a-aab1-2fd18d78a544",
   "metadata": {},
   "source": [
    "To get a visual understanding of how well our model is performing, we can plot a few randomly selected images from the test set along with their true and predicted labels. Here's how we can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326d9bd-bcb2-4fe6-b140-5b18f4217feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Select 10 random images from the test set\n",
    "indices = random.sample(range(len(test_dataset)), 10)\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = axes.flat[i]\n",
    "    img, label = test_dataset[idx]\n",
    "    img = img.permute(1, 2, 0)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{class_idx[label].split('-')[1]}\\nPred: {class_names[y_pred[idx]]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82057a53-87cb-4df4-9336-857ff66016d0",
   "metadata": {},
   "source": [
    "Here, we first select 10 random indices from the test set using the random.sample function. We then loop over these indices and plot the corresponding images along with their true and predicted labels. We use subplots to plot the images in a grid of 2 rows and 5 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d0082-ef24-468b-9ee6-1992595d4510",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099ba32-0033-4524-a506-3a49f1742697",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this tutorial, we have shown how to train a PyTorch model to classify dog breeds using a pre-trained ResNet-18 model as the feature extractor. We have covered all the necessary steps, including loading and preprocessing the data, defining the model architecture, defining the loss function and optimizer, training the model, and testing the model on the test set. With this tutorial, one should be able to train a PyTorch model for their own classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb572f-837b-4a29-a6e2-6f58442c62be",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dltut] *",
   "language": "python",
   "name": "conda-env-dltut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
